import os
import sys
import json
import torch
import hydra
from tqdm import tqdm
from pathlib import Path
from omegaconf import DictConfig, OmegaConf
import math
import re
import numpy as np
import traceback
from PIL import Image

# --- 0. ç¯å¢ƒå˜é‡è‡ªåŠ¨æ³¨å…¥ ---
def setup_environment():
    data_root = os.getenv("NAVSIM_DATA_ROOT")
    if not data_root:
        default_path = "/nfs/dataset-ofs-prediction/rl_lab/leidianqiao/code/recogdrive/data/navsim"
        if os.path.exists(default_path):
            print(f"[Auto-Setup] NAVSIM_DATA_ROOT not set. Using default: {default_path}")
            os.environ["NAVSIM_DATA_ROOT"] = default_path
            data_root = default_path
        else:
            print("[Error] Please export NAVSIM_DATA_ROOT!")
            return False

    if "OPENSCENE_DATA_ROOT" not in os.environ:
        print(f"[Auto-Setup] OPENSCENE_DATA_ROOT not set. Syncing with NAVSIM_DATA_ROOT.")
        os.environ["OPENSCENE_DATA_ROOT"] = data_root

    if "NUPLAN_MAPS_ROOT" not in os.environ:
        maps_path = Path(data_root) / "maps"
        if maps_path.exists():
            print(f"[Auto-Setup] NUPLAN_MAPS_ROOT not set. Auto-setting to: {maps_path}")
            os.environ["NUPLAN_MAPS_ROOT"] = str(maps_path)
        else:
            print(f"[Warning] Maps folder not found at {maps_path}. Map loading might fail!")
    
    return True

if not setup_environment():
    sys.exit(1)

# --- 1. å¯¼å…¥ ---
from navsim.common.dataloader import SceneLoader
from navsim.common.dataclasses import SensorConfig
from navsim.agents.recogdrive.recogdrive_backbone import RecogDriveBackbone
# å¯¼å…¥åº•å±‚å¤„ç†å‡½æ•°
from navsim.agents.recogdrive.utils.internvl_preprocess import dynamic_preprocess, build_transform
from navsim.agents.recogdrive.utils.utils import format_number

# --- 2. è‡ªå®šä¹‰é«˜æ•ˆå›¾ç‰‡åŠ è½½å‡½æ•° ---
def process_image_from_array(image_array, input_size=448, max_num=12):
    image = Image.fromarray(image_array).convert('RGB')
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(image) for image in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values

# --- 3. System Message (ç®€åŒ–ç‰ˆ) ---
REASONING_SYSTEM_MESSAGE = """
You are an autonomous driving assistant. 
Your task is to analyze the scene and explain the expert driver's action concisely.
"""

# --- 4. æ ¸å¿ƒé€»è¾‘å‡½æ•° ---
def get_future_behavior(current_status, future_trajectory):
    if future_trajectory is None: return "Unknown Action"
    if hasattr(future_trajectory, 'poses'): poses = future_trajectory.poses
    else: poses = future_trajectory
    if len(poses) < 5: return "Unknown Action"

    future_idx = min(len(poses) - 1, 29) 
    local_pose = poses[future_idx] 
    dx, dy, d_theta = local_pose[0], local_pose[1], local_pose[2]
    
    dist = math.sqrt(dx**2 + dy**2)
    dt = (future_idx + 1) * 0.1 
    avg_vel = dist / (dt + 1e-6)
    curr_vel = math.sqrt(current_status.ego_velocity[0]**2 + current_status.ego_velocity[1]**2)
    behavior = []

    if avg_vel > curr_vel + 1.5: behavior.append("Accelerate")
    elif avg_vel < curr_vel - 1.5: behavior.append("Decelerate")
    elif curr_vel < 0.5 and avg_vel < 0.5: behavior.append("Remain Stationary")
    else: behavior.append("Maintain Speed")

    if d_theta > 0.15: behavior.append("Turn Left")
    elif d_theta < -0.15: behavior.append("Turn Right")
    else:
        if dy > 1.5: behavior.append("Lane Change Left")
        elif dy < -1.5: behavior.append("Lane Change Right")
        else: behavior.append("Keep Lane")

    return " and ".join(behavior)

def try_repair_response(text):
    """
    æ›´å¼ºå¤§çš„æ¸…æ´—é€»è¾‘ï¼šå»é™¤åµŒå¥—æ ‡ç­¾ï¼Œæå–æ ¸å¿ƒå†…å®¹
    """
    # 1. åŸºç¡€æ¸…æ´—ï¼šå»é™¤å¹»è§‰æ ‡ç­¾å’Œåæ ‡
    text = text.replace("</box>", "").replace("<ref>", "").replace("</ref>", "")
    text = text.replace("</p>", "").replace("<a>", "").replace("</a>", "")
    text = re.sub(r'\[.*?\]', '', text) # å»é™¤æ‰€æœ‰æ–¹æ‹¬å·å†…å®¹ [0, 0.1...]
    
    # 2. æå– Risk (ä¼˜å…ˆæ‰¾æ ‡ç­¾ï¼Œæ‰¾ä¸åˆ°æ‰¾å…³é”®è¯)
    risk = "Unknown"
    risk_match = re.search(r'<risk_level>(.*?)</risk_level>', text, re.DOTALL | re.IGNORECASE)
    if risk_match:
        risk = risk_match.group(1).strip()
    else:
        # å…³é”®è¯å…œåº•
        lower_text = text.lower()
        if "high risk" in lower_text: risk = "High"
        elif "medium risk" in lower_text: risk = "Medium"
        elif "low risk" in lower_text: risk = "Low"
    
    # 3. æå– Perception (è§£å†³åµŒå¥—é—®é¢˜)
    # ç­–ç•¥ï¼šå¦‚æœæ‰¾ä¸åˆ° <perception>ï¼Œå°è¯•æ‰¾ "Perception:" æ–‡æœ¬ï¼Œæˆ–è€…ä» Reasoning é‡Œæ‹†
    perp = "Implied in reasoning"
    perp_match = re.search(r'<perception>(.*?)</perception>', text, re.DOTALL | re.IGNORECASE)
    
    if perp_match:
        perp = perp_match.group(1).strip()
    else:
        # å°è¯•æŸ¥æ‰¾åµŒå¥—åœ¨ reasoning é‡Œçš„ perception
        # æœ‰æ—¶å€™æ¨¡å‹å†™æˆ: <reasoning> <perception> xxx </perception> ...
        pass # æ­£åˆ™å·²ç»è¦†ç›–äº†è¿™ç§æƒ…å†µï¼Œå¦‚æœè¿˜æå–ä¸åˆ°ï¼Œè¯´æ˜çœŸçš„æ²¡å†™

    # 4. æå– Reasoning (å»é™¤åµŒå¥—åœ¨é‡Œé¢çš„å…¶ä»–æ ‡ç­¾)
    reason = text
    reason_match = re.search(r'<reasoning>(.*?)</reasoning>', text, re.DOTALL | re.IGNORECASE)
    if reason_match:
        reason = reason_match.group(1).strip()
        # ã€å…³é”®ã€‘ï¼šå¦‚æœ reasoning é‡Œé¢åŒ…äº† <perception>ï¼ŒæŠŠå®ƒå‰”é™¤æ‰
        reason = re.sub(r'<risk_level>.*?</risk_level>', '', reason, flags=re.DOTALL)
        reason = re.sub(r'<perception>.*?</perception>', '', reason, flags=re.DOTALL)
        reason = reason.strip()
    
    # 5. æœ€ç»ˆæ¸…æ´—ï¼šå»é™¤å¤šä½™æ¢è¡Œ
    perp = " ".join(perp.split())
    reason = " ".join(reason.split())

    return f"<risk_level>{risk}</risk_level>\n<perception>{perp}</perception>\n<reasoning>{reason}</reasoning>", True

# --- ä¸»å…¥å£ ---
@hydra.main(config_path="../../navsim/planning/script/config/common/train_test_split", config_name="navtrain") 
def main(cfg: DictConfig):
    rank = int(os.getenv("RANK", "0"))
    world_size = int(os.getenv("WORLD_SIZE", "1"))
    local_rank = int(os.getenv("LOCAL_RANK", "0"))
    device_str = f"cuda:{local_rank}"
    torch.cuda.set_device(local_rank)
    
    model_path = cfg.get("model_path", "/path/to/your/InternVL-weights")
    if rank == 0:
        print(f"========================================")
        print(f"Process Rank: {rank}/{world_size}")
        print(f">> Model Path: {model_path}")
    
    output_file = f"reasoning_gt_part_{rank}.json" 
    skipped_file = f"skipped_gt_part_{rank}.json"
    data_root = os.getenv("NAVSIM_DATA_ROOT")
    
    # 1. æ¨¡å‹åŠ è½½
    try:
        backbone = RecogDriveBackbone(
            model_type='internvl', checkpoint_path=model_path, device=device_str
        )
        backbone.eval()
        # æ¸…ç©º System Messageï¼Œé˜²æ­¢å¹²æ‰°
        if hasattr(backbone.model, 'system_message'):
            backbone.model.system_message = REASONING_SYSTEM_MESSAGE
    except Exception as e:
        print(f"[Rank {rank}] Model Load Error: {e}")
        return

    # 2. è·¯å¾„å¯¹é½
    navsim_root = Path(data_root)
    log_search_path = navsim_root / "navsim_logs" / "trainval"
    if not log_search_path.exists(): log_search_path = navsim_root / "navsim_logs"
    
    sensor_blobs_path = navsim_root / "sensor_blobs" / "trainval"
    if not sensor_blobs_path.exists(): sensor_blobs_path = navsim_root / "sensor_blobs"

    all_local_logs = list(log_search_path.glob("*.pkl"))
    if len(all_local_logs) == 0:
        print(f"[ERROR] No .pkl files found in {log_search_path}!")
        return

    if rank == 0: 
        print(f"DEBUG: Logs Path: {log_search_path}")
        print(f"DEBUG: Blobs Path: {sensor_blobs_path}")

    all_tokens_str = [f.stem for f in all_local_logs]
    
    # 3. è¿‡æ»¤å™¨è®¾ç½®
    actual_filter = cfg.scene_filter
    if world_size == 1:
        if rank == 0: print(f"DEBUG: Local Mode -> Loading all {len(all_tokens_str)} logs.")
        actual_filter = OmegaConf.create({
            "log_names": all_tokens_str,
            "tokens": None,             
            "scene_blacklist": None,    
            "max_scenes": None,           
            "num_frames": 200,            
            "frame_interval": 200,        
            "start_frame_index": 0,
            "timestamp_threshold_s": None,
            "has_route": False,           
            "num_history_frames": 4,      
            "num_future_frames": 10,
            "min_future_frames": None,
            "camera_type": None,
            "lidar_type": None
        })

    sensor_config = SensorConfig.build_all_sensors()

    # 4. Loader
    scene_loader = SceneLoader(
        sensor_blobs_path=sensor_blobs_path,
        data_path=log_search_path,
        scene_filter=actual_filter, 
        sensor_config=sensor_config, 
    )
    
    all_tokens = scene_loader.tokens
    my_tokens = all_tokens[rank::world_size]
    
    if world_size == 1:
        my_tokens = my_tokens[:5] # ã€æ­£å¼è·‘å…¨é‡æ—¶ï¼Œè¯·æ³¨é‡Šæ‰è¿™è¡Œã€‘
        if rank == 0: print(f"DEBUG: Processing {len(my_tokens)} tokens")

    # 5. Loop
    reasoning_database = {}
    tokens_to_process = my_tokens
    pbar = tqdm(tokens_to_process, desc=f"R{rank}", disable=(rank!=0), position=rank)
    
    valid_count = 0

    for token in pbar:
        try:
            scene = scene_loader.get_scene_from_token(token)
            
            future_trajectory = None
            try: future_trajectory = scene.get_future_trajectory(num_trajectory_frames=30)
            except: pass

            agent_input = scene.get_agent_input()
            gt_action_desc = get_future_behavior(agent_input.ego_statuses[-1], future_trajectory)
            
            if gt_action_desc == "Unknown Action": continue

            # --- å†…å­˜è¯»å– ---
            try:
                cam_data = agent_input.cameras[-1].cam_f0
                image_array = cam_data.image
                
                if True:
                    print(f"\nğŸ“¸ Capturing target image for token: {token}")
                    debug_save_path = f"{token}.jpg"
                    Image.fromarray(image_array).save(debug_save_path)
                    print(f"âœ… Image saved to: {os.path.abspath(debug_save_path)}")
                    print(f"ğŸ‘‰ You can open it in VS Code by running: code {debug_save_path}\n")

                if image_array is None: raise ValueError("Image None")
                pixel_values = process_image_from_array(image_array).to(torch.bfloat16).cuda()
            except Exception as e_img:
                if world_size == 1: print(f"[Skip] Image Error: {e_img}")
                continue

            cmd_idx = 1
            high_command = agent_input.ego_statuses[-1].driving_command
            for i, val in enumerate(high_command):
                if val == 1: cmd_idx = i; break
            command_str = ['TURN LEFT', 'GO STRAIGHT', 'TURN RIGHT'][cmd_idx]

            hist_traj = torch.tensor([[float(e.ego_pose[0]), float(e.ego_pose[1]), float(e.ego_pose[2])] for e in agent_input.ego_statuses[:4]])
            hist_str = " ".join([f't-{3-i}:({format_number(hist_traj[i,0].item())},{format_number(hist_traj[i,1].item())})' for i in range(4)])

            # ================= æ ¸å¿ƒä¿®æ”¹ï¼šONE-SHOT PROMPT =================
            # ç»™å‡ºèŒƒä¾‹ï¼Œå¼ºåˆ¶ç®€æ´ï¼Œå¼ºåˆ¶ç»“æ„
            prompt = (
                f"<image>\n"
                f"You are an AI analyzing human driving.\n"
                f"COMMAND: {command_str}\n"
                f"ACTION: {gt_action_desc}\n\n"
                f"Respond in this EXACT XML format (Concise, <30 words per section):\n"
                f"<risk_level>Low/Medium/High</risk_level>\n"
                f"<perception>Key objects (Traffic light state, Front car status, Obstacles).</perception>\n"
                f"<reasoning>Directly explain why the action was taken based on perception.</reasoning>\n\n"
                f"Example:\n"
                f"<risk_level>Medium</risk_level>\n"
                f"<perception>Red traffic light ahead. Lead vehicle is braking.</perception>\n"
                f"<reasoning>The driver decelerated to stop safely behind the lead vehicle at the red light.</reasoning>\n\n"
                f"Your Output:"
            )

            # é™åˆ¶ max_new_tokens åªæœ‰ 200ï¼Œé€¼è¿«æ¨¡å‹å†™çŸ­å¥
            generation_config = dict(
                num_beams=1, 
                max_new_tokens=256, 
                do_sample=False, 
                repetition_penalty=1.1
            )
            
            response = backbone.model.chat(
                tokenizer=backbone.tokenizer, pixel_values=pixel_values,
                question=prompt, generation_config=generation_config
            )
            
            final_response, _ = try_repair_response(response)
            
            if len(final_response) > 20:
                reasoning_database[token] = final_response
                valid_count += 1
                if valid_count % 10 == 0:
                    with open(output_file, 'w') as f: json.dump(reasoning_database, f, indent=4)

        except Exception as e:
            if world_size == 1: 
                print(f"\n[ERROR] Crash on token {token}:")
                traceback.print_exc()
            continue

    with open(output_file, 'w') as f: json.dump(reasoning_database, f, indent=4)
    if rank == 0: print(f"[Rank 0] Finished. Valid samples: {len(reasoning_database)}")

if __name__ == "__main__":
    main()